This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-12T05:22:00.713Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
atlas/
  core/
    atlas.py
    condition.py
    entity.py
    iquery.py
    pattern.py
  data/
    models.py
    repository.py
  resources/
    api_handler.py
    database_handler.py
    human_interface.py
    llm_handler.py
    openai_handler.py
    prompt_templates.py
    response_processor.py
  utils/
    circuitbreaker.py
    config.py
    logger.py
  __init__.py

================================================================
Repository Files
================================================================

================
File: atlas/core/atlas.py
================
# atlas/core/atlas.py

import asyncio
import logging
from typing import Dict, Any
import networkx as nx
from .entity import Entity
from ..data.repository import Repository
from ..utils.config import config

logger = logging.getLogger(__name__)

class ATLAS:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(ATLAS, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def __init__(self):
        if not self.initialized:
            self.repository = Repository()
            self.entities = {}
            self.global_state = {}
            self.update_interval = config.ATLAS_UPDATE_INTERVAL  # From config
            self.loop = asyncio.get_event_loop()
            self.initialized = True

    def register_entity(self, entity: Entity):
        print(f"Registering ENTITY: {entity.entity_id}")
        self.entities[entity.entity_id] = entity
        logger.debug(f"Entity '{entity.entity_id}' registered with ATLAS. Total entities: {len(self.entities)}")


    def unregister_entity(self, entity_id: str):
        if entity_id in self.entities:
            del self.entities[entity_id]
            logger.debug(f"Entity '{entity_id}' unregistered from ATLAS.")

    async def global_update_cycle(self):
        while True:
            logger.info("Starting global update cycle.")
            tasks = [
                entity.local_update(self.global_state)
                for entity in self.entities.values()
            ]
            await asyncio.gather(*tasks)
            logger.info("Global update cycle completed.")
            await asyncio.sleep(self.update_interval)

    def run(self):
        try:
            self.loop.run_until_complete(self.global_update_cycle())
        except KeyboardInterrupt:
            logger.info("ATLAS stopped by user.")
        finally:
            self.loop.close()

    async def trigger_dynamic_refactor(self):
        """
        Triggers dynamic refactoring of entities based on conditions.
        """
        logger.info("Triggering dynamic refactor.")
        for entity in self.entities.values():
            if self.should_refactor(entity):
                await entity.refactor(self.global_state)

    def should_refactor(self, entity):
        """
        Determines if an entity needs to be refactored.
        """
        return entity.requires_refactor()  # Method within Entity class

    async def manage_autopoiesis(self):
        """
        Manages autopoiesis by generating new entities and patterns.
        """
        logger.info("Managing autopoiesis.")
        for entity in self.entities.values():
            if entity.should_self_generate():
                new_entities = await entity.self_generate(self.global_state)
                for new_entity in new_entities:
                    self.register_entity(new_entity)

    async def global_update_cycle(self):
        """
        Global update cycle now includes dynamic refactoring and autopoiesis.
        """
        while True:
            logger.info("Starting global update cycle.")
            tasks = [entity.local_update(self.global_state) for entity in self.entities.values()]
            await asyncio.gather(*tasks)
            await self.trigger_dynamic_refactor()
            await self.manage_autopoiesis()
            logger.info("Global update cycle completed.")
            await asyncio.sleep(self.update_interval)

    def perform_graph_analysis(self):
        """
        Analyzes the graph structure and performs operations like authority smoothing.
        """
        G = nx.DiGraph()
        for entity in self.entities.values():
            G.add_node(entity.entity_id)
            for reference in entity.references:
                G.add_edge(entity.entity_id, reference)

        hub_scores, authority_scores = nx.hits(G)
        print("SCORES: ", authority_scores.items())
        print("ENTITIES: ", self.entities.items())
        for entity_id, authority in authority_scores.items():
            self.entities[entity_id].attributes['authority'] = authority

    async def smooth_authority(self):
        """
        Smooths authority values between entities in the graph.
        """
        self.perform_graph_analysis()
        min_authority = min(e.attributes['authority'] for e in self.entities.values())
        max_authority = max(e.attributes['authority'] for e in self.entities.values())

        for entity in self.entities.values():
            if entity.attributes['authority'] == min_authority:
                await entity.boost_authority(self.global_state)

    async def global_update_cycle(self):
        """
        Global update cycle now includes dynamic refactoring and autopoiesis.
        """
        while True:
            logger.info("Starting global update cycle.")
            entities_copy = list(self.entities.values())
            tasks = [entity.local_update(self.global_state) for entity in entities_copy]
            await asyncio.gather(*tasks)
            # await self.trigger_dynamic_refactor()
            # await self.manage_autopoiesis()
            # await self.smooth_authority()
            logger.info("Global update cycle completed.")
            await asyncio.sleep(self.update_interval)

================
File: atlas/core/condition.py
================
from abc import ABC, abstractmethod
import operator

class Condition(ABC):
    @abstractmethod
    def evaluate(self, entity, global_state):
        pass

    def __and__(self, other):
        return CompositeCondition(operator.and_, [self, other])

    def __or__(self, other):
        return CompositeCondition(operator.or_, [self, other])

    def __invert__(self):
        return CompositeCondition(operator.not_, [self])

class AttributeCondition(Condition):
    def __init__(self, attribute_name, expected_value, comparison=operator.eq):
        self.attribute_name = attribute_name
        self.expected_value = expected_value
        self.comparison = comparison

    def evaluate(self, entity, global_state):
        actual_value = entity.get_attribute(self.attribute_name)
        return self.comparison(actual_value, self.expected_value)

class GlobalCondition(Condition):
    def __init__(self, global_key, expected_value, comparison=operator.eq):
        self.global_key = global_key
        self.expected_value = expected_value
        self.comparison = comparison

    def evaluate(self, entity, global_state):
        actual_value = global_state.get(self.global_key)
        return self.comparison(actual_value, self.expected_value)

class CompositeCondition(Condition):
    def __init__(self, operator_func, conditions):
        self.operator_func = operator_func
        self.conditions = conditions

    def evaluate(self, entity, global_state):
        results = [cond.evaluate(entity, global_state) for cond in self.conditions]
        return self.operator_func(*results)

================
File: atlas/core/entity.py
================
from ..data.repository import Repository
from typing import Dict, Any
import asyncio

class EntityError(Exception):
    """Custom exception class for Entity-related errors."""
    pass

class Entity:
    def __init__(self, entity_id, patterns=None, attributes=None):
        from .atlas import ATLAS  
        self.repository = Repository()
        self.atlas = ATLAS()
        self.entity_id = entity_id
        self.patterns = patterns or []
        self.iqueries = []
        self.attributes = attributes or {}
        self.references = self.attributes.get('references', [])
        self._persist_entity()
        self.initialize_iqueries()
        self.atlas.register_entity(self)


    def _persist_entity(self):
        existing_entity = self.repository.get_entity_by_id(self.entity_id)
        if existing_entity:
            self.model = existing_entity
            # Update attributes if necessary
            if self.attributes:
                self.model.attributes.update(self.attributes)
                self.model.save()
        else:
            self.model = self.repository.create_entity(self.entity_id, self.attributes)

    def initialize_iqueries(self):
        try:
            for pattern in self.patterns:
                self.iqueries.extend(pattern.get_iqueries())
                self.repository.add_pattern_to_entity(self.model, pattern.model)
            for iquery in self.iqueries:
                self.repository.add_iquery_to_entity(self.model, iquery.model)
        except Exception as e:
            raise EntityError(f"Failed to initialize iQueries: {e}")

    async def local_update(self, global_state):
        for iquery in self.iqueries:
            try:
                if iquery.check_conditions(self, global_state):
                    response = await iquery.execute(self)
                    self.update_attributes_from_response(response)
                    self.repository.update_entity_attributes(self.entity_id, self.attributes)
                    new_references = self.extract_references_from_attributes()
                    self.references.extend(new_references)
            except Exception as e:
                logger.error(f"Error during local update for Entity '{self.entity_id}': {e}")


    def add_pattern(self, pattern):
        if pattern in self.patterns:
            print(f"Pattern '{pattern.name}' is already assigned to Entity '{self.entity_id}'.")
            return
        self.patterns.append(pattern)
        try:
            self.iqueries.extend(pattern.get_iqueries())
            self.repository.add_pattern_to_entity(self.model, pattern.model)
        except Exception as e:
            raise EntityError(f"Failed to add Pattern '{pattern.name}': {e}")

    def add_attribute(self, key, value):
        try:
            self.attributes[key] = value
            self.repository.update_entity_attributes(self.entity_id, {key: value})
        except Exception as e:
            raise EntityError(f"Failed to add/update attribute '{key}': {e}")

    def get_attribute(self, key):
        try:
            return self.attributes.get(key)
        except Exception as e:
            raise EntityError(f"Failed to retrieve attribute '{key}': {e}")

    def remove_pattern(self, pattern):
        if pattern not in self.patterns:
            print(f"Pattern '{pattern.name}' is not assigned to Entity '{self.entity_id}'.")
            return
        self.patterns.remove(pattern)
        # Remove the relationship in the repository
        self.model.patterns.disconnect(pattern.model)
        # Reinitialize iQueries
        self.iqueries = []
        self.initialize_iqueries()

    def remove_attribute(self, key):
        if key in self.attributes:
            del self.attributes[key]
            self.repository.update_entity_attributes(self.entity_id, self.attributes)
        else:
            print(f"Attribute '{key}' does not exist in Entity '{self.entity_id}'.")
    
    def check_and_generate_new_entities(self, global_state):
        """
        Checks conditions and generates new entities based on iQuery responses.
        """
        for iquery in self.iqueries:
            if iquery.check_conditions(self, global_state):
                new_entity_data = iquery.execute(self)
                if new_entity_data:
                    new_entity = EntityFactory.create_entity(new_entity_data)  # Use an Entity Factory for consistency
                    self.atlas.register_entity(new_entity)
                    logger.info(f"Generated new entity: {new_entity.entity_id}")


    async def local_update(self, global_state):
        for iquery in self.iqueries:
            try:
                if iquery.check_conditions(self, global_state):
                    new_entity_data = await iquery.execute(self)
                    # Update the entity's attributes in the repository
                    self.repository.update_entity_attributes(self.entity_id, self.attributes)
                    if new_entity_data:
                        self.generate_new_entities(new_entity_data)
            except Exception as e:
                print(f"Error during local update for Entity '{self.entity_id}': {e}")
    
    def generate_new_entities(self, new_entity_data_list):
        for data in new_entity_data_list:
            new_entity = EntityFactory.create_entity(data)
            self.atlas.register_entity(new_entity)
            logger.info(f"Generated new entity: {new_entity.entity_id}")
    
    def update_attributes_from_response(self, response):
        """
        Update entity attributes based on the response from an iQuery.
        """
        if response and isinstance(response, dict):
            # Update attributes with new data
            self.attributes.update(response)
        else:
            print(f"Invalid response format for entity '{self.entity_id}'.")

def extract_references_from_attributes(self):
    """
    Extract references from attributes after iQuery execution.
    """
    # Assuming the response includes a 'references' field with new references
    new_references = self.attributes.get('references', [])
    # Ensure references are unique
    return [ref for ref in new_references if ref not in self.references]



class EntityFactory:
    """
    A factory to manage the creation of new entities.
    This ensures uniform creation logic and avoids inconsistencies.
    """
    @staticmethod
    def create_entity(entity_data):
        entity_id = entity_data['entity_id']
        patterns = entity_data.get('patterns', [])
        attributes = entity_data.get('attributes', {})
        return Entity(entity_id=entity_id, patterns=patterns, attributes=attributes)

================
File: atlas/core/iquery.py
================
import time
import random
import json
import logging
import asyncio
from functools import reduce
from operator import and_, or_
from typing import Any

from ..data.repository import Repository
from ..data.models import IQueryModel, ResourceHandlerModel

class iQuery:
    MAX_RETRIES = 3
    BACKOFF_FACTOR = 2  # Exponential backoff factor
    VALID_STATUSES = {'pending', 'executing', 'completed', 'failed', 'retrying'}
    
    def __init__(self, name, target_attribute, resource_handlers, conditions=None):
        self.repository = Repository()
        self.name = name
        self.target_attribute = target_attribute
        self.resource_handlers = resource_handlers  # List of handler instances
        self.resource_handler_models = [handler.resource_handler_model for handler in resource_handlers]  # Extract models
        self.conditions = conditions or []
        self.status = 'pending'
        self.retry_count = 0
        self._persist_iquery()

    def _persist_iquery(self):
        existing_iquery = IQueryModel.nodes.get_or_none(name=self.name)
        if existing_iquery:
            self.model = existing_iquery
        else:
            self.model = self.repository.create_iquery(
                self.name,
                self.target_attribute,
                self.conditions,
                self.status
            )
        for handler_model in self.resource_handler_models:
            self.repository.add_resource_handler_to_iquery(self.model, handler_model)

    def check_conditions(self, entity, global_state):
        if not self.conditions:
            return True
        return self.conditions.evaluate(entity, global_state)

    async def execute(self, entity):
        logging.info(f"Executing IQuery '{self.name}' for entity {entity}")
        self.status = 'executing'
        self.model.status = self.status
        self.model.save()
        handlers = self.resource_handlers.copy()
        while handlers and self.retry_count <= self.MAX_RETRIES:
            handler = handlers.pop(0)
            try:
                query = self.build_query(entity)
                logging.debug(f"Built query: {query}")
                response = await handler.execute(query)
                logging.debug(f"Received response: {response}")
                if response:
                    # Use the processed response directly
                    attribute_value, new_entity_data = self.process_response(response)
                    entity.add_attribute(self.target_attribute, attribute_value)
                    self.status = 'completed'
                    self.model.status = self.status
                    self.model.save()
                    logging.info(f"IQuery '{self.name}' completed successfully")
                    return attribute_value
            except Exception as e:
                logging.error(f"Error with handler '{handler}': {str(e)}", exc_info=True)
                self.retry_count += 1
                if self.retry_count > self.MAX_RETRIES:
                    if handlers:
                        logging.warning(f"Falling back to next handler for IQuery '{self.name}'")
                        self.status = 'retrying'
                        self.retry_count = 0  # Reset for next handler
                        continue
                    else:
                        logging.error(f"No more handlers to try. Marking IQuery '{self.name}' as failed")
                        self.status = 'failed'
                        self.model.status = self.status
                        self.model.save()
                        return
                backoff_time = self.BACKOFF_FACTOR ** self.retry_count + random.uniform(0, 1)
                logging.info(f"Retrying with handler '{handler}' in {backoff_time:.2f} seconds...")
                self.status = 'retrying'
                self.model.status = self.status
                self.model.save()
                await asyncio.sleep(backoff_time)
            self.status = 'failed'
            self.model.status = self.status
            self.model.save()
            logging.error(f"IQuery '{self.name}' failed after all retries")

    def build_query(self, entity):
        # Custom implementation as needed
        return f"Provide {self.target_attribute} for {entity.entity_id}"

    def update_status(self, new_status):
        if new_status not in self.VALID_STATUSES:
            raise ValueError(f"Invalid status '{new_status}' for iQuery.")
        self.status = new_status
        self.model.status = self.status
        self.model.save()
        
    def process_response(self, response):
        # Example processing logic
        # Assume response is a dictionary with keys 'attribute_value' and 'new_entities'
        attribute_value = response.get('attribute_value')
        new_entities = response.get('new_entities', [])
        return attribute_value, new_entities

================
File: atlas/core/pattern.py
================
from ..data.repository import Repository
from .entity import Entity
import logging

logger = logging.getLogger(__name__)


class PatternConsistencyError(Exception):
    """Custom exception for consistency-related issues in Patterns."""
    pass

class Pattern(Entity):
    def __init__(self, name, iqueries=None, parent_patterns=None, attributes=None):
        """
        Patterns now inherit from Entity, enabling them to have iQueries, attributes,
        and self-generation behavior.
        """
        super().__init__(entity_id=name, patterns=parent_patterns, attributes=attributes)
        self.name = name  # Add this line to set the name attribute
        self.iqueries = iqueries or []
        self.parent_patterns = parent_patterns or []
        self._persist_pattern()

    def _persist_pattern(self):
        """
        Persists the pattern in the repository.
        """
        existing_pattern = self.repository.get_pattern_by_name(self.name)
        if existing_pattern:
            self.model = existing_pattern
        else:
            self.model = self.repository.create_pattern(self.name)
        for iquery in self.iqueries:
            self.repository.add_iquery_to_pattern(self.model, iquery.model)
        for parent_pattern in self.parent_patterns:
            self.model.parent_patterns.connect(parent_pattern.model)

    def get_iqueries(self):
        """
        Returns all iQueries, including inherited ones from parent patterns.
        """
        inherited_iqueries = []
        for parent in self.parent_patterns:
            inherited_iqueries.extend(parent.get_iqueries())
        return inherited_iqueries + self.iqueries

    def add_iquery(self, iquery):
        """
        Adds an iQuery to the pattern.
        """
        if iquery in self.iqueries:
            print(f"iQuery '{iquery.name}' already exists in Pattern '{self.name}'.")
            return
        self.iqueries.append(iquery)
        self.repository.add_iquery_to_pattern(self.model, iquery.model)

    def inherit_from(self, parent_pattern):
        """
        Allows the pattern to inherit from a parent pattern.
        """
        if parent_pattern in self.parent_patterns:
            print(f"Pattern '{parent_pattern.name}' is already a parent of Pattern '{self.name}'.")
            return
        self.parent_patterns.append(parent_pattern)
        self.model.parent_patterns.connect(parent_pattern.model)

    def validate_consistency(self):
        """
        Checks for conflicting iQueries or circular inheritance.
        """
        visited = set()
        def dfs(pattern):
            if pattern in visited:
                raise PatternConsistencyError(f"Circular inheritance detected in pattern '{pattern.name}'")
            visited.add(pattern)
            for parent in pattern.parent_patterns:
                dfs(parent)
        dfs(self)
        logger.info(f"Pattern '{self.name}' passed consistency validation.")

    async def generate_new_pattern(self, global_state):
        """
        Generates new patterns based on existing patterns and global state.
        """
        new_iqueries = self.get_iqueries()
        new_pattern_name = f"{self.name}_variant"
        new_pattern = Pattern(name=new_pattern_name, iqueries=new_iqueries, parent_patterns=[self])
        logger.info(f"Generated new pattern: {new_pattern.name}")
        return new_pattern

    def self_modify(self, global_state):
        """
        Logic for self-modification based on global state or conditions.
        """
        # Placeholder for modification logic based on global state
        pass

    async def local_update(self, global_state):
        """
        Updates the pattern locally and handles self-modification and generation of new patterns.
        """
        await super().local_update(global_state)
        self.self_modify(global_state)
        await self.generate_new_pattern(global_state)

================
File: atlas/data/models.py
================
# atlas/data/models.py

from neomodel import (
    StructuredNode,
    StringProperty,
    JSONProperty,
    RelationshipTo,
    RelationshipFrom,
    UniqueIdProperty,
)

class ResourceHandlerModel(StructuredNode):
    uid = UniqueIdProperty()
    handler_type = StringProperty(required=True)
    config = JSONProperty(default={})

    # Relationships
    iqueries = RelationshipFrom('IQueryModel', 'USES_HANDLER')

class IQueryModel(StructuredNode):
    uid = UniqueIdProperty()
    name = StringProperty(required=True)
    target_attribute = StringProperty(required=True)
    conditions = JSONProperty(default=[])
    status = StringProperty(default='pending')

    # Relationships
    resource_handlers = RelationshipTo('ResourceHandlerModel', 'USES_HANDLER')
    entity = RelationshipFrom('EntityModel', 'HAS_IQUERY')
    patterns = RelationshipFrom('PatternModel', 'HAS_IQUERY')

class PatternModel(StructuredNode):
    uid = UniqueIdProperty()
    name = StringProperty(unique_index=True, required=True)

    # Relationships
    iqueries = RelationshipTo('IQueryModel', 'HAS_IQUERY')
    parent_patterns = RelationshipTo('PatternModel', 'INHERITS_FROM')
    entities = RelationshipFrom('EntityModel', 'HAS_PATTERN')

class EntityModel(StructuredNode):
    uid = UniqueIdProperty()
    entity_id = StringProperty(unique_index=True, required=True)
    attributes = JSONProperty(default={})

    # Relationships
    patterns = RelationshipTo('PatternModel', 'HAS_PATTERN')
    iqueries = RelationshipTo('IQueryModel', 'HAS_IQUERY')

================
File: atlas/data/repository.py
================
from .models import EntityModel, PatternModel, IQueryModel, ResourceHandlerModel
from cachetools import cached, TTLCache

class Repository:
    def __init__(self):
        # Cache with a Time-To-Live of 300 seconds and max size of 100 entries
        self.entity_cache = TTLCache(maxsize=100, ttl=300)

    @cached(cache=lambda self: self.entity_cache)
    def get_entity_by_id(self, entity_id):
        return EntityModel.nodes.get_or_none(entity_id=entity_id)

    def create_entity(self, entity_id, attributes=None):
        entity = EntityModel(entity_id=entity_id, attributes=attributes or {})
        entity.save()
        return entity

    def get_entity_by_id(self, entity_id):
        return EntityModel.nodes.get_or_none(entity_id=entity_id)

    def update_entity_attributes(self, entity_id, new_attributes):
        entity = self.get_entity_by_id(entity_id)
        if entity:
            entity.attributes.update(new_attributes)
            entity.save()
            return entity
        return None

    def delete_entity(self, entity_id):
        entity = self.get_entity_by_id(entity_id)
        if entity:
            entity.delete()

    def create_pattern(self, name):
        pattern = PatternModel(name=name)
        pattern.save()
        return pattern

    def get_pattern_by_name(self, name):
        return PatternModel.nodes.get_or_none(name=name)

    def add_pattern_to_entity(self, entity, pattern):
        entity.patterns.connect(pattern)

    def create_iquery(self, name, target_attribute, conditions=None, status='pending'):
        iquery = IQueryModel(
            name=name,
            target_attribute=target_attribute,
            conditions=conditions or [],
            status=status
        )
        iquery.save()
        return iquery

    def add_iquery_to_entity(self, entity, iquery):
        entity.iqueries.connect(iquery)

    def add_iquery_to_pattern(self, pattern, iquery):
        pattern.iqueries.connect(iquery)

    def create_resource_handler(self, handler_type, config=None):
        handler = ResourceHandlerModel(handler_type=handler_type, config=config or {})
        handler.save()
        return handler

    def add_resource_handler_to_iquery(self, iquery, handler):
        iquery.resource_handlers.connect(handler)

    def batch_create_entities(self, entities_data):
        cypher_query = """
        UNWIND $batch as row
        CREATE (e:EntityModel {entity_id: row.entity_id, attributes: row.attributes})
        RETURN e
        """
        
        results, meta = db.cypher_query(cypher_query, {'batch': entities_data})
        return [EntityModel.inflate(row[0]) for row in results]
    
    def get_resource_handler_by_type(self, handler_type):
        return ResourceHandlerModel.nodes.get_or_none(handler_type=handler_type)

    def create_resource_handler(self, handler_type, config):
        handler = ResourceHandlerModel(handler_type=handler_type, config=config)
        handler.save()
        return handler

================
File: atlas/resources/api_handler.py
================
# atlas/resources/api_handler.py

import aiohttp
import asyncio
from abc import ABC, abstractmethod
import logging
from typing import Any, Optional


logger = logging.getLogger(__name__)

class APIHandler(ABC):
    @abstractmethod
    async def get(self, endpoint: str, params: Dict[str, Any] = None) -> Any:
        pass

    @abstractmethod
    async def post(self, endpoint: str, data: Dict[str, Any] = None) -> Any:
        pass

class ExternalAPIHandler(APIHandler):
    def __init__(self, base_url: str, api_key: Optional[str] = None):
        self.base_url = base_url
        self.api_key = api_key
        self.session = aiohttp.ClientSession()

    async def get(self, endpoint: str, params: Dict[str, Any] = None) -> Any:
        url = f"{self.base_url}{endpoint}"
        headers = {
            'Authorization': f'Bearer {self.api_key}',
        }
        try:
            async with self.session.get(url, headers=headers, params=params, timeout=10) as response:
                response.raise_for_status()
                data = await response.json()
                return data
        except aiohttp.ClientResponseError as e:
            logger.error(f"HTTP error during API GET request: {e}")
        except Exception as e:
            logger.exception(f"Unexpected error during API GET request: {e}")
        return None

    async def post(self, endpoint: str, data: Dict[str, Any] = None) -> Any:
        # Similar implementation for POST requests
        pass

    async def close(self):
        await self.session.close()

================
File: atlas/resources/database_handler.py
================
# atlas/resources/database_handler.py

import asyncpg
import logging
from abc import ABC, abstractmethod
from typing import Any, List

logger = logging.getLogger(__name__)

class DatabaseHandler(ABC):
    @abstractmethod
    async def fetch(self, query: str, *args) -> List[Any]:
        pass

    @abstractmethod
    async def execute(self, query: str, *args) -> None:
        pass

class AsyncPGDatabaseHandler(DatabaseHandler):
    def __init__(self, dsn: str):
        self.dsn = dsn
        self.pool = None

    async def initialize(self):
        self.pool = await asyncpg.create_pool(dsn=self.dsn, min_size=1, max_size=10)

    async def fetch(self, query: str, *args) -> List[Any]:
        if not self.pool:
            await self.initialize()
        try:
            async with self.pool.acquire() as connection:
                result = await connection.fetch(query, *args)
                return result
        except Exception as e:
            logger.exception(f"Error fetching from database: {e}")
            return []

    async def execute(self, query: str, *args) -> None:
        if not self.pool:
            await self.initialize()
        try:
            async with self.pool.acquire() as connection:
                await connection.execute(query, *args)
        except Exception as e:
            logger.exception(f"Error executing database command: {e}")

    async def close(self):
        if self.pool:
            await self.pool.close()

================
File: atlas/resources/human_interface.py
================
import asyncio
import logging
from abc import ABC, abstractmethod
from typing import Any, Optional

logger = logging.getLogger(__name__)

class HumanInterfaceHandler(ABC):
    @abstractmethod
    async def send_request(self, prompt: str) -> None:
        pass

    @abstractmethod
    async def receive_response(self) -> Optional[str]:
        pass

class AsyncHumanInterfaceHandler(HumanInterfaceHandler):
    def __init__(self, queue: asyncio.Queue):
        self.queue = queue

    async def send_request(self, prompt: str) -> None:
        # Simulate sending a request to a human operator
        await self.queue.put(prompt)

    async def receive_response(self) -> Optional[str]:
        # Simulate receiving a response from a human operator
        response = await self.queue.get()
        return response

================
File: atlas/resources/llm_handler.py
================
from abc import ABC, abstractmethod
from ..data.repository import Repository
from ..data.models import ResourceHandlerModel
from typing import Any, Optional

class LLMHandler(ABC):
    def __init__(self, handler_type='LLM', config=None):
        self.repository = Repository()
        self.handler_type = handler_type
        self.config = config or {}

class LLMHandler(ABC):
    def __init__(self, handler_type='LLM', config=None):
        self.repository = Repository()
        self.handler_type = handler_type
        self.config = config or {}
        self.model = None  # This will be set in _persist_handler

    @abstractmethod
    def _persist_handler(self):
        pass

    @abstractmethod
    async def execute(self, prompt: str, **kwargs) -> Optional[str]:
        pass

    @abstractmethod
    def build_prompt(self, entity: 'Entity', iquery: 'iQuery') -> str:
        pass

    @abstractmethod
    def process_response(self, response: str) -> Any:
        pass

================
File: atlas/resources/openai_handler.py
================
import aiohttp
import asyncio
import logging
import json
from typing import Any, Dict, Optional
from .llm_handler import LLMHandler
from atlas.utils.config import config
from atlas.core.entity import Entity
from atlas.core.iquery import iQuery
from .llm_handler import LLMHandler
from .response_processor import ResponseProcessor

logger = logging.getLogger(__name__)

from .llm_handler import LLMHandler
from ..data.models import ResourceHandlerModel
from ..utils.config import config
from ..data.repository import Repository

class OpenAIGPTHandler(LLMHandler):
    def __init__(self):
        super().__init__(handler_type='OpenAI', config={'model': config.OPENAI_MODEL})
        self.api_key = config.OPENAI_API_KEY
        self.api_base_url = config.OPENAI_API_BASE_URL
        self.model_name = config.OPENAI_MODEL  # Keep the model name string
        self.session = aiohttp.ClientSession()
        self.semaphore = asyncio.Semaphore(5)
        self.response_processor = ResponseProcessor()
        self._persist_handler()

    def _persist_handler(self):
        repository = Repository()
        existing_handler = repository.get_resource_handler_by_type('OpenAI')
        if existing_handler:
            self.resource_handler_model = existing_handler  # Store the ResourceHandlerModel instance
        else:
            self.resource_handler_model = repository.create_resource_handler('OpenAI', {'model': self.model_name})



    async def execute(self, prompt: str, **kwargs) -> Optional[Dict[str, Any]]:
        """
        Asynchronously sends the prompt to OpenAI's API and returns the processed response.
        """
        url = f"{self.api_base_url}/chat/completions"
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json',
        }
        payload = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': kwargs.get('temperature', 0.7),
            'max_tokens': kwargs.get('max_tokens', 150),
            'n': 1,
            'stop': kwargs.get('stop', None),
        }

        max_retries = kwargs.get('max_retries', 3)
        backoff_factor = kwargs.get('backoff_factor', 0.5)
        logger.debug(f"Starting request with max_retries={max_retries}, backoff_factor={backoff_factor}")
        for attempt in range(max_retries):
            async with self.semaphore:
                try:
                    logger.debug(f"Attempt {attempt + 1}: Sending request to {url}")
                    logger.debug(f"Payload: {json.dumps(payload, indent=2)}")
                    logger.debug(f"Headers: {json.dumps(headers, indent=2)}")
                    async with self.session.post(url, json=payload, headers=headers, timeout=15) as response:
                        response.raise_for_status()
                        data = await response.json()
                        logger.debug(f"Received response: {json.dumps(data, indent=2)}")
                        text_response = data['choices'][0]['message']['content'].strip()
                        processed_data = self.process_response(text_response)
                        logger.debug(f"Processed data: {json.dumps(processed_data, indent=2)}")
                        return processed_data  # Return processed data (dictionary)
                except aiohttp.ClientResponseError as e:
                    if e.status in [429, 500, 502, 503, 504]:
                        delay = backoff_factor * (2 ** attempt) + random.uniform(0, 0.1)
                        logger.warning(f"Retrying after {delay:.2f} seconds due to {e.status} error")
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"Non-retriable HTTP error: {e}")
                        break
                except json.JSONDecodeError as e:
                    logger.error(f"JSON decode error: {e}")
                    logger.debug(f"Raw response: {await response.text()}")
                    break
                except Exception as e:
                    logger.exception(f"Unexpected error during LLM request: {e}")
                    break
        logger.error("All attempts failed")
        return None


    def build_prompt(self, entity: Entity, iquery: iQuery) -> str:
        """
        Constructs the prompt based on the entity's attributes and the iQuery's parameters.
        """
        prompt_template = iquery.parameters.get('prompt_template')
        if not prompt_template:
            raise ValueError("Prompt template not found in iQuery parameters.")

        # Advanced context integration
        context = entity.get_context()
        prompt = prompt_template.format(**entity.attributes, context=context)
        return prompt

    def process_response(self, response: str) -> Any:
        """
        Processes and validates the LLM response.
        """
        if self.response_processor.validate_response(response):
            return self.response_processor.extract_information(response)
        else:
            logger.warning("Invalid response received from LLM.")
            return None

    async def close(self):
        await self.session.close()

================
File: atlas/resources/prompt_templates.py
================
from string import Template

class PromptTemplates:
    @staticmethod
    def get_definition_prompt(entity: 'Entity') -> str:
        """
        Generates a prompt for obtaining the definition of an entity.
        """
        template_str = (
            "As an expert in ${domain}, provide a detailed and precise definition of '${name}'. "
            "Include relevant context and examples where appropriate."
        )
        template = Template(template_str)
        context = entity.get_context()  # Assume this method provides additional context
        return template.substitute(name=entity.attributes.get('name'), domain=context.get('domain', 'the subject'))

================
File: atlas/resources/response_processor.py
================
import spacy
from typing import Dict, Any

class ResponseProcessor:
    def __init__(self):
        self.nlp = spacy.load('en_core_web_sm')

    def validate_response(self, response: str) -> bool:
        doc = self.nlp(response)
        # Check for specific entities or linguistic features
        if len(doc) > 0:
            return True
        return False

    def extract_information(self, response: str) -> Dict[str, Any]:
        doc = self.nlp(response)
        # Extract relevant information from the response
        # Implement custom logic as needed
        return {"text": response}

================
File: atlas/utils/circuitbreaker.py
================
# atlas/utils/circuit_breaker.py

import time
import logging

logger = logging.getLogger(__name__)

class CircuitBreaker:
    def __init__(self, max_failures=5, reset_timeout=60):
        self.max_failures = max_failures
        self.reset_timeout = reset_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'

    def call(self, func, *args, **kwargs):
        if self.state == 'OPEN':
            if time.time() - self.last_failure_time > self.reset_timeout:
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit is open")

        try:
            result = func(*args, **kwargs)
            self._reset()
            return result
        except Exception as e:
            self._record_failure()
            raise e

    def _reset(self):
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'

    def _record_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.max_failures:
            self.state = 'OPEN'
            logger.warning("Circuit breaker opened")

================
File: atlas/utils/config.py
================
import os
from pydantic import Field
from pydantic_settings import BaseSettings
from neomodel import config as neomodel_config

class Settings(BaseSettings):
    ATLAS_UPDATE_INTERVAL: int = Field(default=60, env='ATLAS_UPDATE_INTERVAL')
    OPENAI_API_KEY: str = Field(..., env='OPENAI_API_KEY')
    OPENAI_API_BASE_URL: str = 'https://api.openai.com/v1'
    OPENAI_MODEL: str = 'gpt-4'

    # Neo4j settings
    NEO4J_USERNAME: str = Field(default='neo4j', env='NEO4J_USERNAME')
    NEO4J_PASSWORD: str = Field(..., env='NEO4J_PASSWORD')
    NEO4J_HOST: str = Field(default='localhost', env='NEO4J_HOST')
    NEO4J_PORT: int = Field(default=7687, env='NEO4J_PORT')

    class Config:
        env_file = os.path.join(os.path.dirname(__file__), '..', '.env')
        env_file_encoding = 'utf-8'

    @property
    def neo4j_database_url(self):
        return f"bolt://{self.NEO4J_USERNAME}:{self.NEO4J_PASSWORD}@{self.NEO4J_HOST}:{self.NEO4J_PORT}"

# Initialize settings
config = Settings()

# Configure neomodel
neomodel_config.DATABASE_URL = config.neo4j_database_url

print(f"Neo4j Database URL: {config.neo4j_database_url}")  # For debugging

================
File: atlas/utils/logger.py
================
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

================
File: atlas/__init__.py
================
from .core import *
from .data import *
from .resources import *
from .interfaces import *
from .utils import *
